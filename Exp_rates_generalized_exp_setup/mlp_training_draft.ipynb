{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9deef77f-bf73-41f8-a605-033e589eaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from dataset_reader import Traces_Dataset\n",
    "# from mlp_model import MLP\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d549acc2-c09b-4827-bee2-d19cd070fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: d6d3e3f7\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# initialize a dictionary of training history to store in a csv file\n",
    "history_dict = {}\n",
    "\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "history_dict['unique_id'] = unique_id\n",
    "print(f'Experiment ID: {unique_id}')\n",
    "\n",
    "# making training reproducible\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "history_dict['seed'] = seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Uncomment this line\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d4db0a-8743-4888-b966-216d5e640e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000000, 321])\n"
     ]
    }
   ],
   "source": [
    "# load and process dataset \n",
    "dataset = Traces_Dataset('dataset2mil.csv')\n",
    "# dataset = Traces_Dataset('dataset_test.csv')\n",
    "dataset.split_dataset(0.95, 0.05, 0)\n",
    "dataset.clean_features()\n",
    "dataset.find_mean_std()\n",
    "dataset.normalize()\n",
    "print(dataset.inputs.shape)\n",
    "history_dict['normalize_mean'] = dataset.train_mean.tolist()\n",
    "history_dict['normalize_std'] = dataset.train_std.tolist()\n",
    "history_dict['dataset'] = (dataset.inputs.shape[0], dataset.inputs.shape[1])\n",
    "\n",
    "# initialize train, val, test set\n",
    "X_train = dataset[dataset.train_set.indices][0]\n",
    "Y_train = dataset[dataset.train_set.indices][1]\n",
    "\n",
    "X_val = dataset[dataset.val_set.indices][0]\n",
    "Y_val = dataset[dataset.val_set.indices][1]\n",
    "\n",
    "X_test = dataset[dataset.test_set.indices][0]\n",
    "Y_test = dataset[dataset.test_set.indices][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e2bc94f-79f5-44aa-bc8f-06a1e17bb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): \n",
    "    def __init__(self, n_features, n_params=7):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(n_features, 125),\n",
    "                nn.BatchNorm1d(125),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(125, 125),\n",
    "                nn.BatchNorm1d(125),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(125, 125),\n",
    "                nn.BatchNorm1d(125),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(125, 125),\n",
    "                nn.BatchNorm1d(125),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(125, n_params)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Apply Xavier initialization to linear layers\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "                # You can also initialize biases, for example, with zeros\n",
    "                if layer.bias is not None:\n",
    "                    init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e484136c-ee6c-4d20-ad8d-32483b0aec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NN model\n",
    "model = MLP(dataset.inputs.shape[1], dataset.params.shape[1]).to(device)\n",
    "model.initialize_weights()\n",
    "\n",
    "# store model architecture\n",
    "architecture = []\n",
    "for name, layer in model.named_children():\n",
    "    architecture.append(layer)\n",
    "history_dict['architecture'] = architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f42d67ba-6501-4755-ac5e-74b2376975d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=321, out_features=125, bias=True)\n",
      "    (1): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=125, out_features=125, bias=True)\n",
      "    (4): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=125, out_features=125, bias=True)\n",
      "    (7): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=125, out_features=125, bias=True)\n",
      "    (10): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=125, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a4f5305-b8cf-4da2-be1f-c2ff3ba2e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001) \n",
    "\n",
    "# record optimizer\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "history_dict['optimizer'] = f'{optimizer.__class__.__name__} {lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8b451c0-2bce-41fa-8c84-b5123234d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 89382\n"
     ]
    }
   ],
   "source": [
    "# functions to save and load best nn model, and create an unique id to store the model\n",
    "def checkpoint(model, filename, folder='models'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    return filepath\n",
    "def resume(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "# this is a 1d vector containing all weight in the nn model, no bias included. \n",
    "def weights_1d(model): \n",
    "    # Get parameters with gradients\n",
    "    parameters_with_grad = [param for param in model.parameters() if param.requires_grad]\n",
    "    # Concatenate parameters into a 1D tensor\n",
    "    flat_parameters = torch.cat([param.view(-1) for param in parameters_with_grad])\n",
    "    return flat_parameters\n",
    "print(f'Number of model parameters: {weights_1d(model).shape[0]}')\n",
    "weights_change_updates = np.array([])  # using numpy array to save memory \n",
    "weights_change_epochs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8aca4285-133f-4843-97d9-4dae0819884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_epochs = 300   # number of epochs to run\n",
    "batch_size = 1024  # size of each batch\n",
    "history_dict['epochs'] = n_epochs\n",
    "history_dict['batch size'] = batch_size\n",
    "\n",
    "# initialize dataloader \n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialization train, val losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# record the weights with no training\n",
    "# with torch.no_grad():\n",
    "#     previous_weights_epoch = weights_1d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a516ebbf-60e6-4d1c-a112-ac0d6344af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: train-579.0567193298505, val-19.596047712832082\n",
      "2: train-13.89483207104535, val-9.642817039879\n",
      "3: train-9.45454570676746, val-10.227993147713798\n",
      "4: train-7.82828332894835, val-9.791195781863465\n",
      "5: train-6.736649763995204, val-9.405744795896569\n",
      "6: train-6.051027098349456, val-10.758202689034599\n",
      "7: train-5.423118731328126, val-5.832976136888776\n",
      "8: train-5.016837889906661, val-9.267020303375867\n",
      "9: train-4.63913432662857, val-8.532382673146774\n",
      "10: train-4.267233007170003, val-6.088935229243065\n",
      "11: train-4.0087227920262976, val-4.162988711376579\n",
      "12: train-3.8057663104143638, val-10.483747131970464\n",
      "13: train-3.6236545795510553, val-6.080276902841062\n",
      "14: train-3.3985254903291833, val-3.580239792259372\n",
      "15: train-3.272798581883825, val-4.756282694485723\n",
      "16: train-3.177991761408489, val-14.982386044093541\n",
      "17: train-3.0592096365089048, val-3.773691089785829\n",
      "18: train-2.9238000280255902, val-3.206713077973346\n",
      "19: train-2.8361674919082174, val-20.06652561499148\n",
      "20: train-2.801461866382381, val-6.7680666398028935\n",
      "21: train-2.7134799021456777, val-2.3541222421490415\n",
      "22: train-2.6307687391366423, val-4.282413146933731\n",
      "23: train-2.5728685449295003, val-6.571425403867449\n",
      "24: train-2.4966585842314464, val-2.60432869074296\n",
      "25: train-2.4602120814780735, val-4.145151349962974\n",
      "26: train-2.3871288864777007, val-14.585158114530602\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m      4\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre06/project/6000803/maxwell8/Compute-Canada-Research/ENV/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # record the weights changes across each updates (batches)\n",
    "        # with torch.no_grad():\n",
    "        #     current_weights = weights_1d(model)\n",
    "        #     if 'previous_weights' in locals():\n",
    "        #         weight_change = torch.norm(current_weights - previous_weights).item()#.cpu()\n",
    "        #         weights_change_updates = np.append(weights_change_updates, weight_change)\n",
    "        #     previous_weights = current_weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average training loss for the epoch\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        # record the weights change across epochs\n",
    "        # current_weights_epoch = weights_1d(model)\n",
    "        # if 'previous_weights_epoch' in locals():\n",
    "        #     weight_change_epoch = torch.norm(current_weights_epoch - previous_weights_epoch).item()#.cpu()\n",
    "        #     weights_change_epochs.append(weight_change_epoch)\n",
    "        # previous_weights_epoch = current_weights_epoch\n",
    "\n",
    "        # validation\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = loss_fn(val_outputs, val_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'{epoch}: train-{avg_train_loss}, val-{avg_val_loss}')\n",
    "\n",
    "    # saving the model with best training mse\n",
    "    # if len(train_losses) != 1:\n",
    "    #     if avg_train_loss < train_losses[-2]: \n",
    "    #         best_loss = avg_train_loss\n",
    "    #         best_epoch = epoch\n",
    "    #         model_path = checkpoint(model, f\"model_{unique_id}.pth\")\n",
    "    if avg_val_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        model_path = checkpoint(model, f\"model_{unique_id}.pth\")\n",
    "        best_training_loss = avg_train_loss\n",
    "        best_validation_loss = avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9094ae1-b720-43ef-9d4c-165328dadc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record training, validationg losses, weight updates, and the result model path\n",
    "history_dict['best_epoch'] = best_epoch\n",
    "history_dict['best_val'] = best_validation_loss\n",
    "history_dict['best_train'] = best_training_loss\n",
    "history_dict['training'] = train_losses\n",
    "history_dict['validation'] = val_losses\n",
    "history_dict['weights'] = weights_change_epochs\n",
    "history_dict['model'] = model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14369135-9904-49e3-a7dc-6c89c063b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_records(row_data, file_path = 'experiment records.csv'): \n",
    "    '''\n",
    "    row_data is a dictionary of the row_data we want to store\n",
    "    '''\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # If the file is newly created, write the header row\n",
    "        if not file_exists:\n",
    "            header_row = row_data.keys() if isinstance(row_data, dict) else row_data\n",
    "            csv_writer.writerow(header_row)\n",
    "\n",
    "        # Write the data row\n",
    "        if isinstance(row_data, dict):\n",
    "            csv_writer.writerow(row_data.values())\n",
    "        else:\n",
    "            csv_writer.writerow(row_data)\n",
    "\n",
    "\n",
    "print(f\"Best validation loss: {best_validation_loss}, at epoch: {best_epoch}\")\n",
    "#print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "print('Number of total training samples: ', X_train.shape[0])\n",
    "\n",
    "experiment_records(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66796a5b-2c1b-45a6-8493-cec11c9580c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
