{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9deef77f-bf73-41f8-a605-033e589eaa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# from mlp_model import MLP\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "sys.path.append('..')\n",
    "from dataset_reader import Traces_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d549acc2-c09b-4827-bee2-d19cd070fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 465af071\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# initialize a dictionary of training history to store in a csv file\n",
    "history_dict = {}\n",
    "\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "history_dict['unique_id'] = unique_id\n",
    "print(f'Experiment ID: {unique_id}')\n",
    "\n",
    "# making training reproducible\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "history_dict['seed'] = seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Uncomment this line\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d4db0a-8743-4888-b966-216d5e640e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000000, 321])\n"
     ]
    }
   ],
   "source": [
    "# load and process dataset \n",
    "dataset = Traces_Dataset('../dataset2mil.csv')\n",
    "# dataset = Traces_Dataset('dataset_test.csv')\n",
    "dataset.split_dataset(0.95, 0.05, 0)\n",
    "dataset.clean_features()\n",
    "dataset.find_mean_std()\n",
    "dataset.normalize()\n",
    "print(dataset.inputs.shape)\n",
    "history_dict['normalize_mean'] = dataset.train_mean.tolist()\n",
    "history_dict['normalize_std'] = dataset.train_std.tolist()\n",
    "history_dict['dataset'] = (dataset.inputs.shape[0], dataset.inputs.shape[1])\n",
    "\n",
    "# initialize train, val, test set\n",
    "X_train = dataset[dataset.train_set.indices][0]\n",
    "Y_train = dataset[dataset.train_set.indices][1]\n",
    "\n",
    "X_val = dataset[dataset.val_set.indices][0]\n",
    "Y_val = dataset[dataset.val_set.indices][1]\n",
    "\n",
    "X_test = dataset[dataset.test_set.indices][0]\n",
    "Y_test = dataset[dataset.test_set.indices][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2bc94f-79f5-44aa-bc8f-06a1e17bb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): \n",
    "    def __init__(self, n_features, n_params=7):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(n_features, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(512, n_params)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Apply Xavier initialization to linear layers\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "                # You can also initialize biases, for example, with zeros\n",
    "                if layer.bias is not None:\n",
    "                    init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c47ad213-d8f3-43cb-b48a-9f715e26d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size = 321, output_size = 7):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_hidden_layers = len(hidden_sizes)\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Define the input layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_sizes[0]),\n",
    "                nn.SiLU(),  # SILU activation function\n",
    "                nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "        # Define the hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),\n",
    "                nn.SiLU(),  # SILU activation function\n",
    "                nn.BatchNorm1d(hidden_sizes[i+1])\n",
    "            )\n",
    "            for i in range(len(hidden_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "        # Define the output layer\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Apply Xavier initialization to linear layers\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "                # Initialize biases, for example, with zeros\n",
    "                if layer.bias is not None:\n",
    "                    init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e484136c-ee6c-4d20-ad8d-32483b0aec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NN model\n",
    "# model = MLP(dataset.inputs.shape[1], dataset.params.shape[1]).to(device)\n",
    "model=FeedForwardNN((512,512,512)).to(device)\n",
    "model.initialize_weights()\n",
    "\n",
    "# store model architecture\n",
    "architecture = []\n",
    "for name, layer in model.named_children():\n",
    "    architecture.append(layer)\n",
    "history_dict['architecture'] = architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f42d67ba-6501-4755-ac5e-74b2376975d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=321, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a4f5305-b8cf-4da2-be1f-c2ff3ba2e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001) \n",
    "\n",
    "# record optimizer\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "history_dict['optimizer'] = f'{optimizer.__class__.__name__} {lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8b451c0-2bce-41fa-8c84-b5123234d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 696839\n"
     ]
    }
   ],
   "source": [
    "# functions to save and load best nn model, and create an unique id to store the model\n",
    "def checkpoint(model, filename, folder='models'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    return filepath\n",
    "def resume(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "# this is a 1d vector containing all weight in the nn model, no bias included. \n",
    "def weights_1d(model): \n",
    "    # Get parameters with gradients\n",
    "    parameters_with_grad = [param for param in model.parameters() if param.requires_grad]\n",
    "    # Concatenate parameters into a 1D tensor\n",
    "    flat_parameters = torch.cat([param.view(-1) for param in parameters_with_grad])\n",
    "    return flat_parameters\n",
    "print(f'Number of model parameters: {weights_1d(model).shape[0]}')\n",
    "weights_change_updates = np.array([])  # using numpy array to save memory \n",
    "weights_change_epochs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aca4285-133f-4843-97d9-4dae0819884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_epochs = 300   # number of epochs to run\n",
    "batch_size = 1024  # size of each batch\n",
    "history_dict['epochs'] = n_epochs\n",
    "history_dict['batch size'] = batch_size\n",
    "\n",
    "# initialize dataloader \n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialization train, val losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# record the weights with no training\n",
    "# with torch.no_grad():\n",
    "#     previous_weights_epoch = weights_1d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a516ebbf-60e6-4d1c-a112-ac0d6344af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: train-299.99125144707745, val-13.499434325159813\n",
      "2: train-6.246581845756235, val-13.745541339017906\n",
      "3: train-4.609669863535412, val-5.126841978150971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # record the weights changes across each updates (batches)\n",
    "        # with torch.no_grad():\n",
    "        #     current_weights = weights_1d(model)\n",
    "        #     if 'previous_weights' in locals():\n",
    "        #         weight_change = torch.norm(current_weights - previous_weights).item()#.cpu()\n",
    "        #         weights_change_updates = np.append(weights_change_updates, weight_change)\n",
    "        #     previous_weights = current_weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average training loss for the epoch\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        # record the weights change across epochs\n",
    "        # current_weights_epoch = weights_1d(model)\n",
    "        # if 'previous_weights_epoch' in locals():\n",
    "        #     weight_change_epoch = torch.norm(current_weights_epoch - previous_weights_epoch).item()#.cpu()\n",
    "        #     weights_change_epochs.append(weight_change_epoch)\n",
    "        # previous_weights_epoch = current_weights_epoch\n",
    "\n",
    "        # validation\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = loss_fn(val_outputs, val_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'{epoch}: train-{avg_train_loss}, val-{avg_val_loss}')\n",
    "\n",
    "    # saving the model with best training mse\n",
    "    # if len(train_losses) != 1:\n",
    "    #     if avg_train_loss < train_losses[-2]: \n",
    "    #         best_loss = avg_train_loss\n",
    "    #         best_epoch = epoch\n",
    "    #         model_path = checkpoint(model, f\"model_{unique_id}.pth\")\n",
    "    if avg_val_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        model_path = checkpoint(model, f\"model_{unique_id}.pth\")\n",
    "        best_training_loss = avg_train_loss\n",
    "        best_validation_loss = avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9094ae1-b720-43ef-9d4c-165328dadc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record training, validationg losses, weight updates, and the result model path\n",
    "history_dict['best_epoch'] = best_epoch\n",
    "history_dict['best_val'] = best_validation_loss\n",
    "history_dict['best_train'] = best_training_loss\n",
    "history_dict['training'] = train_losses\n",
    "history_dict['validation'] = val_losses\n",
    "history_dict['weights'] = weights_change_epochs\n",
    "history_dict['model'] = model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14369135-9904-49e3-a7dc-6c89c063b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_records(row_data, file_path = 'experiment records.csv'): \n",
    "    '''\n",
    "    row_data is a dictionary of the row_data we want to store\n",
    "    '''\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # If the file is newly created, write the header row\n",
    "        if not file_exists:\n",
    "            header_row = row_data.keys() if isinstance(row_data, dict) else row_data\n",
    "            csv_writer.writerow(header_row)\n",
    "\n",
    "        # Write the data row\n",
    "        if isinstance(row_data, dict):\n",
    "            csv_writer.writerow(row_data.values())\n",
    "        else:\n",
    "            csv_writer.writerow(row_data)\n",
    "\n",
    "\n",
    "print(f\"Best validation loss: {best_validation_loss}, at epoch: {best_epoch}\")\n",
    "#print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "print('Number of total training samples: ', X_train.shape[0])\n",
    "\n",
    "experiment_records(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66796a5b-2c1b-45a6-8493-cec11c9580c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
