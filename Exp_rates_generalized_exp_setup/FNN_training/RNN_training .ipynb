{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2597545-d224-4bd8-a10a-cad30346c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import sys  # Import the sys module\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "sys.path.append('..')\n",
    "from dataset_reader import Traces_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e877205-b4ac-4aee-9da3-1403f629f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making training reproducible\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427027db-0323-4c31-84ef-c45ace48038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000000, 329])\n"
     ]
    }
   ],
   "source": [
    "# load and process dataset \n",
    "dataset = Traces_Dataset('../dataset2mil.csv')\n",
    "dataset.split_dataset(0.9, 0.1, 0)\n",
    "# dataset.clean_features()\n",
    "dataset.find_mean_std()\n",
    "dataset.normalize()\n",
    "print(dataset.inputs.shape)\n",
    "# history_dict['normalize_mean'] = dataset.train_mean.tolist()\n",
    "# history_dict['normalize_std'] = dataset.train_std.tolist()\n",
    "# history_dict['dataset'] = (dataset.inputs.shape[0], dataset.inputs.shape[1])\n",
    "\n",
    "# initialize train, val, test set\n",
    "X_train = dataset[dataset.train_set.indices][0]\n",
    "Y_train = dataset[dataset.train_set.indices][1]\n",
    "\n",
    "X_val = dataset[dataset.val_set.indices][0]\n",
    "Y_val = dataset[dataset.val_set.indices][1]\n",
    "\n",
    "X_test = dataset[dataset.test_set.indices][0]\n",
    "Y_test = dataset[dataset.test_set.indices][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2693ce35-0449-4601-afcf-2f6ade9eb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_train = X_train[:, :9]\n",
    "traces_train = X_train[:, 8:]\n",
    "setup_val = X_val[:, :9]\n",
    "traces_val = X_val[:, 9:]\n",
    "setup_train.shape, traces_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edc70130-3f8d-4015-9a20-ad4fa6929216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1800000, 20, 16]), torch.Size([200000, 20, 16]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model RNN parameters\n",
    "def rnn_add_dim(inputs, n_points=20, n_traces=16): \n",
    "    '''\n",
    "    adding a dim to dataset, default = 2, so theres one channel for time, one for current\n",
    "    '''\n",
    "    return inputs.reshape(-1, n_traces, n_points).transpose(1,2)\n",
    "\n",
    "\n",
    "traces_train_transformed = rnn_add_dim(traces_train)\n",
    "traces_val_transformed = rnn_add_dim(traces_val)\n",
    "traces_train_transformed.shape, traces_val_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa3d9-b314-455e-8e7f-2905b36dbf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c23c9dd7-108a-4258-8d79-bfb0b84c3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_MLP_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, mlp_hidden_size, output_size):\n",
    "        super(RNN_MLP_Model, self).__init__()\n",
    "        \n",
    "        # Define the RNN layer (LSTM in this case)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size + 9, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_setup = x[:, :9]\n",
    "        x_traces = x[:, 9:]\n",
    "        \n",
    "        # Forward pass through the RNN layer\n",
    "        _, (h_n, _) = self.rnn(rnn_add_dim(x_traces))\n",
    "        \n",
    "        # Extract the hidden state from the last time step\n",
    "        rnn_output = h_n[-1]\n",
    "        \n",
    "        # Forward pass through the MLP layers\n",
    "        # print(torch.cat((x_setup, rnn_output), dim=1).shape)\n",
    "        output = self.mlp(torch.cat((x_setup, rnn_output), dim=1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75a4fde6-e428-46f5-a37a-3890796757fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model hyper-parameters\n",
    "input_size = 16 # numer of traces times 2\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "mlp_hidden_size = 64\n",
    "n_classes = 7 # 8 parameters to estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23b2b8a1-6ffe-434f-a79b-5c51d4074cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Uncomment this line\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "541b25ae-13d1-4b50-adac-27a7f297981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NN model\n",
    "model = RNN_MLP_Model(input_size, hidden_size, n_layers, mlp_hidden_size, n_classes).to(device)\n",
    "model.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db0522b5-1bfd-429d-bb05-e58ee705e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_epochs = 300   # number of epochs to run\n",
    "batch_size = 1024  # size of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7f2f0abf-a8b5-41c0-acb8-334734087bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloader \n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f7c7b55-931f-4c4a-bc7b-abae4d7095db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7545d-1988-46f8-b291-294775220fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: train-55.83905145820904, val-20.65707439305831\n",
      "2: train-19.354117458590874, val-18.579236935596075\n",
      "3: train-13.806811706469192, val-11.718442148091842\n",
      "4: train-10.112605646066156, val-7.8259642197161305\n",
      "5: train-8.73757749721323, val-5.922453607831683\n",
      "6: train-8.071302918443907, val-17.13266634941101\n",
      "7: train-7.612460662887365, val-5.399930688799644\n",
      "8: train-6.949658720295313, val-5.5169393772981605\n",
      "9: train-6.899972063960747, val-7.69943198135921\n",
      "10: train-6.1565974343216325, val-3.9842091859603417\n",
      "11: train-5.2785491171686045, val-4.057454382886692\n",
      "12: train-4.452463392918427, val-3.165950312906382\n",
      "13: train-3.9544196187161478, val-4.262090083287687\n",
      "14: train-3.75137394110633, val-2.9680109644422727\n",
      "15: train-6.56616749962849, val-24.310941715629735\n",
      "16: train-17.67979163331519, val-13.67090985726337\n",
      "17: train-10.546148637862743, val-18.345007925617452\n",
      "18: train-7.998585707227253, val-8.187169291535202\n",
      "19: train-7.0207163848160885, val-9.160915866190074\n",
      "20: train-6.1770203041677725, val-4.931701888843459\n",
      "21: train-5.465114385737222, val-3.1974111162886327\n",
      "22: train-5.359569036513059, val-4.88496285555314\n",
      "23: train-4.666106138213096, val-4.656501778534481\n",
      "24: train-4.177506053542657, val-3.1751090373311723\n",
      "25: train-4.050286166361439, val-2.546805665809281\n",
      "26: train-3.8117735935152592, val-2.578634662287576\n",
      "27: train-3.6196685120107372, val-3.6593441342820925\n",
      "28: train-3.517586219907485, val-10.756377385587108\n",
      "29: train-3.366665080206657, val-3.341973232979677\n",
      "30: train-3.1081535287955786, val-2.5211796882201214\n",
      "31: train-2.953711285051361, val-3.212253390526285\n",
      "32: train-2.8613307107837533, val-2.51912929938764\n",
      "33: train-2.8308127348197876, val-1.8640494456096572\n",
      "34: train-2.6861853836881964, val-2.569473624229431\n",
      "35: train-2.6009002348939982, val-2.7130251283548317\n"
     ]
    }
   ],
   "source": [
    "# initialization train, val losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "    # Average training loss for the epoch\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        # validation\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = loss_fn(val_outputs, val_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'{epoch}: train-{avg_train_loss}, val-{avg_val_loss}')\n",
    "\n",
    "    if avg_val_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        # model_path = checkpoint(model, f\"model_{unique_id}.pth\")\n",
    "        best_training_loss = avg_train_loss\n",
    "        best_validation_loss = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3e033-7381-4e9f-80a3-02225d3e491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record training, validationg losses, weight updates, and the result model path\n",
    "history_dict['best_epoch'] = best_epoch\n",
    "history_dict['best_val'] = best_validation_loss\n",
    "history_dict['best_train'] = best_training_loss\n",
    "history_dict['training_loss'] = train_losses\n",
    "history_dict['validation_loss'] = val_losses\n",
    "\n",
    "for history in history_dict:\n",
    "    print(f'{history}: {history_dict[history]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
